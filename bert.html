<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BERT &mdash; BERT on SQuAD 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=d45e8c67"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="_static/copybutton.js?v=6dbb43f8"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="BERT for Question Answering" href="bert-qa.html" />
    <link rel="prev" title="Kubeflow Pipelines" href="kubeflow-pipelines.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            BERT on SQuAD
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guides:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="local-setup.html">Local Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="kubeflow-pipelines.html">Kubeflow Pipelines</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Conceptual Guides:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">BERT</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#architecture">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#objective">Objective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pre-training">Pre-training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning">Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bert-qa.html">BERT for Question Answering</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BERT on SQuAD</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">BERT</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/bert.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bert">
<h1>BERT<a class="headerlink" href="#bert" title="Link to this heading"></a></h1>
<p>BERT (Bidirectional Encoder Representations from Transformers) is a
revolutionary model developed by Google in 2018. Its introduction marked a
significant advancement in the field, setting new state-of-the-art benchmarks
across various NLP tasks.</p>
<p>BERT is pre-trained on a massive amount of data, acquiring a sense of what
language is and what’s the meaning of context in a document. Then, this
pre-trained model can then be fine-tuned for specific tasks such as question
answering or sentiment analysis.</p>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Link to this heading"></a></h2>
<p>BERT’s architecture is based on the Transformer model, which has been
particularly influential in the realm of deep learning for NLP tasks.</p>
<p><img alt="BERT Architecture" src="_images/bert-architecture.png" /></p>
<p>Here’s a breakdown of the key components of BERT’s architecture:</p>
<ul class="simple">
<li><p><strong>Transformer Architecture</strong>: BERT utilizes the encoder portion of the
Transformer architecture. The Transformer model itself is made of an
encoder-decoder stack, but BERT only uses the encoder stack.</p></li>
<li><p><strong>Embeddings</strong>: Input tokens (words or subwords) are transformed into
embeddings, which are then fed into the model. BERT combines both token and
positional embeddings as input.</p></li>
<li><p><strong>Positional Encodings</strong>: Since BERT and the underlying Transformer
architecture do not have any built-in sense of word order (as recurrent models
like LSTMs do), they incorporate positional encodings to give the model
information about the position of words in a sequence.</p></li>
<li><p><strong>Attention Mechanism</strong>: One of the primary innovations in the Transformer
architecture is the “self-attention mechanism” which allows the model to weigh
the importance of different words in a sentence relative to a given word,
thereby capturing context. This is crucial for BERT’s bidirectionality.</p></li>
<li><p><strong>Feed-Forward Neural Networks</strong>: Each Transformer block contains a
feed-forward neural network that operates independently on each position.</p></li>
<li><p><strong>Layer Normalization &amp; Residual Connections</strong>: Each sub-layer (like
self-attention or feed-forward neural network) in the model includes a
residual connection around it followed by layer normalization. This helps in
training deep networks by mitigating the vanishing gradient problem.</p></li>
<li><p><strong>Multiple Stacks</strong>: BERT’s depth is one of its defining characteristics. The
“base” version of BERT uses 12 stacked Transformer encoders, while the “large”
version uses 24.</p></li>
</ul>
</section>
<section id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Link to this heading"></a></h2>
<p>We usually split the training of BERT in two phases: the first phase is
pre-training, where the model understands language and context, while the second
phase is fine-tuning, where the model learns a specific task.</p>
<section id="pre-training">
<h3>Pre-training<a class="headerlink" href="#pre-training" title="Link to this heading"></a></h3>
<p>In pre-training, BERT tries to solve two tasks simultaneously:</p>
<ul class="simple">
<li><p>Masked Language Model (MLM or “cloze” test)</p></li>
<li><p>Next Sentence Prediction (NSP).</p></li>
</ul>
<blockquote>
<div><p>The word <em><a class="reference external" href="https://en.wiktionary.org/wiki/cloze" title="wikt:cloze">cloze</a></em> is
derived from <em>closure</em> in <a class="reference external" href="https://en.wikipedia.org/wiki/Gestalt_psychology" title="Gestalt psychology">Gestalt theory</a>.</p>
</div></blockquote>
<p>In the first paradigm, random words in a sentence are replaced with a <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>
token, and BERT tries to predict the original word from the context.
This is different from traditional language models, which predict words in a
sequence.</p>
<p>For the second task, BERT takes in two sentences, and it determines if the
second sentence follows the first. This helps BERT understand context across
sentences. Also, this is where segment embeddings become critical, as they allow
the model to differentiate between the two sentences. A pair of sentences fed
into BERT for NSP will be assigned different segment embeddings to indicate to
the model which sentence each token belongs to.</p>
</section>
<section id="fine-tuning">
<h3>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading"></a></h3>
<p>Once pre-trained, we can specialize BERT on a specific task using a smaller
labeled dataset. These tasks can range from sentiment analysis to question
answering.</p>
<p>To understand how BERT can be fine-tuned for a QA task, visit the <a class="reference internal" href="bert-qa.html"><span class="std std-doc">BERT for
Question Answering</span></a> page.</p>
</section>
</section>
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f">Training BERT from scratch: Introduction</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822">The BERT Tokenizer</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5">Preparing Data for BERT</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb">Pre-training BERT</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/attention-please-25b2933309f4">The Attention Mechanism</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="kubeflow-pipelines.html" class="btn btn-neutral float-left" title="Kubeflow Pipelines" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bert-qa.html" class="btn btn-neutral float-right" title="BERT for Question Answering" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Dimitris Poulopoulos.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>