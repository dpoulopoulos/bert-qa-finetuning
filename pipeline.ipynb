{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d1a612-3e8d-4a4d-93ec-1aea206aab72",
   "metadata": {},
   "source": [
    "# Scale the Experiment on Kubeflow\n",
    "\n",
    "This Notebook scales the fine-tuning experiment to a [Kubeflow](https://www.kubeflow.org/) cluster by breaking the process into distinct steps and running them in a Directed Acyclic Graph (DAG) using [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/).\n",
    "\n",
    "There are several advantages to this approach:\n",
    "\n",
    "- ðŸš€ Scale and accelerate the experiment by leveraging more resources and more powerful machines.\n",
    "- ðŸŽ Parallelize steps that can run independently.\n",
    "- ðŸ«™ Cache steps, such as data processing, to avoid repeating them on each run.\n",
    "- ðŸ“… Schedule recurring runs to retrain your model whenever new data is available.\n",
    "- ðŸ“ˆ Track and visualize the experiment's parameters, outputs, and performance metrics, making it easier to compare and debug.\n",
    "- âœ¨ Automate model deployment by integrating KFP with CI/CD pipelines once.\n",
    "\n",
    "The following pipeline comprises three steps:\n",
    "\n",
    "- ðŸ”» Download the data and model in a specific PVC. These steps run in parallel.\n",
    "- ðŸ”¨ Preprocess the `train` and `validation` data splits. The steps run in parallel.\n",
    "- ðŸŽµ Fine-tune the model and store in in a different PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5c447-f59e-4606-ad94-4a1fec88e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "from kfp import dsl, compiler, kubernetes, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53435f-e199-43a7-9de7-d3e1c6c74b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"/\")\n",
    "SERVICE_ACCOUNT = ROOT/Path(\"var/run/secrets/kubernetes.io/serviceaccount\")\n",
    "NAMESPACE = open(SERVICE_ACCOUNT/\"namespace\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38362f71-71c2-4ceb-9078-94ac0fac5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['datasets==2.21.0'])\n",
    "def download_data(\n",
    "    dataset_id: Optional[str] = \"rajpurkar/squad\") -> None:\n",
    "    \"\"\"Download the SQuAD dataset to a specified location.\n",
    "\n",
    "    Args:\n",
    "        dataset_id: The name of the dataset variant. Default is \"rajpurkar/squad\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.environ[\"HF_DATASETS_DOWNLOADED_DATASETS_PATH\"] = os.path.join(\"/huggingface\", \"datasets\", \"squad\")\n",
    "\n",
    "    import datasets\n",
    "\n",
    "    datasets.load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62501cc5-3475-42dd-8819-a82607f5f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['transformers[torch]==4.44.2'])\n",
    "def download_model(model_id: Optional[str] = \"google-bert/bert-base-uncased\") -> None:\n",
    "    \"\"\"Download the BERT model to a specified location.\n",
    "\n",
    "    Args:\n",
    "        model_id: The name of the model variant. Default is \"google-bert/bert-base-uncased\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.environ[\"HF_HUB_CACHE\"] = os.path.join(\"/huggingface\", \"hub\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62776d7d-8256-4040-8b10-d844ef854723",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['datasets==2.21.0', 'transformers==4.44.2'])\n",
    "def process_training_data(\n",
    "    dataset_id: Optional[str] = \"rajpurkar/squad\",\n",
    "    model_id: Optional[str] = \"google-bert/bert-base-uncased\",\n",
    "    huggingface_cache: Optional[str] = \"/huggingface\"\n",
    ") -> None:\n",
    "    \"\"\"Process the train split of the SQuAD dataset and store it to a specified location.\n",
    "\n",
    "    Args:\n",
    "        dataset_id: The name of the dataset variant. Default is \"rajpurkar/squad\".\n",
    "        model_id: The name of the model variant. Default is \"google-bert/bert-base-uncased\".\n",
    "        huggingface_cache: The path to store the processed dataset. Default is /data/processed_train.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from functools import partial\n",
    "    from pathlib import Path\n",
    "\n",
    "    os.environ[\"HF_HUB_CACHE\"] = os.path.join(\"/huggingface\", \"hub\")\n",
    "    os.environ[\"HF_DATASETS_DOWNLOADED_DATASETS_PATH\"] = os.path.join(\"/huggingface\", \"datasets\", \"squad\")\n",
    "\n",
    "    import datasets\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    data = datasets.load_dataset(dataset_id, split=\"train\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    output_path = Path(huggingface_cache) / \"datasets\" / \"squad_processed\"\n",
    "    \n",
    "    def _preprocess(examples, tokenizer, max_length, stride):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "          questions,\n",
    "          examples[\"context\"],\n",
    "          truncation=\"only_second\",\n",
    "          padding=\"max_length\",\n",
    "          stride=stride,\n",
    "          max_length=max_length,\n",
    "          return_offsets_mapping=True,\n",
    "          return_overflowing_tokens=True,\n",
    "        )\n",
    "\n",
    "        answers = examples[\"answers\"]\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "        sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            sample_idx = sample_map[i]\n",
    "            answer = answers[sample_idx]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    preprocess_data = partial(_preprocess, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "    processed_data = data.map(preprocess_data, batched=True, remove_columns=data.column_names)\n",
    "    processed_data.save_to_disk(os.path.join(output_path, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1908b5d-43ee-47cb-a191-23f57585812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['datasets==2.21.0', 'transformers==4.44.2'])\n",
    "def process_validation_data(\n",
    "    dataset_id: Optional[str] = \"rajpurkar/squad\",\n",
    "    model_id: Optional[str] = \"google-bert/bert-base-uncased\",\n",
    "    huggingface_cache: Optional[str] = \"/huggingface\"\n",
    ") -> None:\n",
    "    \"\"\"Process the train split of the SQuAD dataset and store it to a specified location.\n",
    "\n",
    "    Args:\n",
    "        dataset_id: The name of the dataset variant. Default is \"rajpurkar/squad\".\n",
    "        model_id: The name of the model variant. Default is \"google-bert/bert-base-uncased\".\n",
    "        huggingface_cache: The path to store the processed dataset. Default is /data/processed_train.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from functools import partial\n",
    "    from pathlib import Path\n",
    "    \n",
    "    os.environ[\"HF_HUB_CACHE\"] = os.path.join(\"/huggingface\", \"hub\")\n",
    "    os.environ[\"HF_DATASETS_DOWNLOADED_DATASETS_PATH\"] = os.path.join(\"/huggingface\", \"datasets\", \"squad\")\n",
    "\n",
    "    import datasets\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    data = datasets.load_dataset(dataset_id, split=\"validation\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    output_path = Path(huggingface_cache) / \"datasets\" / \"squad_processed\"\n",
    "    \n",
    "    def _preprocess(examples, tokenizer, max_length, stride):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        inputs = tokenizer(\n",
    "          questions,\n",
    "          examples[\"context\"],\n",
    "          truncation=\"only_second\",\n",
    "          padding=\"max_length\",\n",
    "          stride=stride,\n",
    "          max_length=max_length,\n",
    "          return_offsets_mapping=True,\n",
    "          return_overflowing_tokens=True,\n",
    "        )\n",
    "\n",
    "        example_ids = []\n",
    "        answers = examples[\"answers\"]\n",
    "        offset_mapping = inputs[\"offset_mapping\"]\n",
    "        sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offset in enumerate(offset_mapping):\n",
    "            sample_idx = sample_map[i]\n",
    "            example_ids.append(examples[\"id\"][sample_idx])\n",
    "            answer = answers[sample_idx]\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            offset = inputs[\"offset_mapping\"][i]\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "        inputs[\"example_id\"] = example_ids\n",
    "        inputs[\"start_positions\"] = start_positions\n",
    "        inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    preprocess_data = partial(_preprocess, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "    processed_data = data.map(preprocess_data, batched=True, remove_columns=data.column_names)\n",
    "    processed_data.save_to_disk(os.path.join(output_path, \"validation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adcf26-deb0-4683-b6ab-9f37a31b89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['datasets==2.21.0', 'transformers[torch]==4.44.2', 'tensorboard==2.14.0'])\n",
    "def fine_tune(\n",
    "    model_id: Optional[str] = \"google-bert/bert-base-uncased\",\n",
    "    huggingface_cache: Optional[str] = \"/huggingface\",\n",
    "    logging_steps: Optional[int] = 100,\n",
    "    save_steps: Optional[int] = 200,\n",
    "    eval_strategy: Optional[str] = \"steps\",\n",
    "    logging_strategy: Optional[str] = \"steps\",\n",
    "    save_strategy: Optional[str] = \"steps\",\n",
    "    learning_rate: Optional[float] = 3e-5,\n",
    "    num_train_epochs: Optional[int] = 2,\n",
    "    weight_decay: Optional[float] = 1e-2,\n",
    "    use_bf16: Optional[bool] = True,\n",
    "    per_device_train_batch_size: Optional[int] = 32,\n",
    "    per_device_eval_batch_size: Optional[int] = 64\n",
    ") -> None:\n",
    "    \"\"\"Fine-tune BERT on the SQuAD dataset.\n",
    "\n",
    "    Args:\n",
    "        model_id: The name of the model variant. Default is \"google-bert/bert-base-uncased\".\n",
    "        huggingface_cache: The path to store the HuggingFace assets. Default is /huggingface.\n",
    "        logging_steps: The number of steps between each log. Default is 200.\n",
    "        save_steps: The number of steps between each model checkpoint save. Default is 200.\n",
    "        eval_strategy: The evaluation strategy to use during training. Default is \"steps\".\n",
    "        logging_strategy: The logging strategy to use during training. Default is \"steps\".\n",
    "        save_strategy: The checkpoint saving strategy to use during training. Default is \"steps\".\n",
    "        learning_rate: The initial learning rate for training. Default is 3e-5.\n",
    "        num_train_epochs: The number of epochs to train the model. Default is 2.\n",
    "        weight_decay: The weight decay to apply during optimization to prevent overfitting. Default is 1e-2.\n",
    "        use_bf16: Whether to use bf16 (Brain Float 16) precision during training. Default is True.\n",
    "        per_device_train_batch_size: The batch size for training per device (e.g., per GPU/TPU). Default is 32.\n",
    "        per_device_eval_batch_size: The batch size for evaluation per device (e.g., per GPU/TPU). Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    os.environ[\"HF_HUB_CACHE\"] = os.path.join(\"/huggingface\", \"hub\")\n",
    "    os.environ[\"HF_DATASETS_DOWNLOADED_DATASETS_PATH\"] = os.path.join(\"/huggingface\", \"datasets\", \"squad\")\n",
    "\n",
    "    import datasets\n",
    "    from transformers import Trainer, TrainingArguments\n",
    "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    data_path = Path(huggingface_cache) / \"datasets\" / \"squad_processed\"\n",
    "    train_data = datasets.load_from_disk(os.path.join(data_path, \"train\"))\n",
    "    valid_data = datasets.load_from_disk(os.path.join(data_path, \"validation\"))\n",
    "\n",
    "    logs_dir = os.path.join(\"/runs\", \"logs\")\n",
    "    checkpoints_dir = os.path.join(\"/runs\", \"checkpoints\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_dir,\n",
    "        logging_dir=logs_dir,\n",
    "        eval_strategy=eval_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        logging_strategy=logging_strategy,\n",
    "        save_steps=save_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        bf16=use_bf16,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=valid_data,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(os.path.join(\"runs\", \"trained_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217f662-1fa1-4fed-85a7-5f7619f448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def bert_squad_pipeline(\n",
    "    namespace: str,\n",
    "    dataset_id: Optional[str] = \"rajpurkar/squad\",\n",
    "    model_id: Optional[str] = \"google-bert/bert-base-uncased\",\n",
    "    huggingface_cache: Optional[str] = \"/huggingface\",\n",
    "    logging_steps: Optional[int] = 100,\n",
    "    save_steps: Optional[int] = 200,\n",
    "    eval_strategy: Optional[str] = \"steps\",\n",
    "    logging_strategy: Optional[str] = \"steps\",\n",
    "    save_strategy: Optional[str] = \"steps\",\n",
    "    learning_rate: Optional[float] = 3e-5,\n",
    "    num_train_epochs: Optional[int] = 2,\n",
    "    weight_decay: Optional[float] = 1e-2,\n",
    "    use_bf16: Optional[bool] = True,\n",
    "    per_device_train_batch_size: Optional[int] = 32,\n",
    "    per_device_eval_batch_size: Optional[int] = 64\n",
    ") -> None:\n",
    "    \"\"\"Define a KFP Pipeline for downloading competition data and launching a distributed training job.\n",
    "\n",
    "    Args:\n",
    "        dataset_id: The name of the dataset variant. Default is \"rajpurkar/squad\".\n",
    "        model_id: The name of the model variant. Default is \"google-bert/bert-base-uncased\".\n",
    "        huggingface_cache: The path to store the HuggingFace assets. Default is /huggingface.\n",
    "        logging_steps: The number of steps between each log. Default is 200.\n",
    "        save_steps: The number of steps between each model checkpoint save. Default is 200.\n",
    "        eval_strategy: The evaluation strategy to use during training. Default is \"steps\".\n",
    "        logging_strategy: The logging strategy to use during training. Default is \"steps\".\n",
    "        save_strategy: The checkpoint saving strategy to use during training. Default is \"steps\".\n",
    "        learning_rate: The initial learning rate for training. Default is 3e-5.\n",
    "        num_train_epochs: The number of epochs to train the model. Default is 2.\n",
    "        weight_decay: The weight decay to apply during optimization to prevent overfitting. Default is 1e-2.\n",
    "        use_bf16: Whether to use bf16 (Brain Float 16) precision during training. Default is True.\n",
    "        per_device_train_batch_size: The batch size for training per device (e.g., per GPU/TPU). Default is 32.\n",
    "        per_device_eval_batch_size: The batch size for evaluation per device (e.g., per GPU/TPU). Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # create a PVC to store the dataset\n",
    "    huggingface_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='huggingface',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='8.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "    \n",
    "    # create a PVC to store the run logs and assets\n",
    "    bert_squad_pvc = kubernetes.CreatePVC(\n",
    "        pvc_name='bert-squad',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='8.0Gi',\n",
    "        storage_class_name='longhorn'\n",
    "    )\n",
    "\n",
    "    download_data_step = download_data(dataset_id=dataset_id).after(huggingface_pvc)\n",
    "    download_data_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    download_model_step = download_model(model_id=model_id).after(huggingface_pvc)\n",
    "    download_model_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    process_training_data_step = process_training_data(\n",
    "        dataset_id=dataset_id, model_id=model_id, huggingface_cache=huggingface_cache).after(download_data_step)\n",
    "    process_training_data_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    process_validation_data_step = process_validation_data(\n",
    "        dataset_id=dataset_id, model_id=model_id, huggingface_cache=huggingface_cache).after(download_data_step)\n",
    "    process_validation_data_step.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    fine_tune_task = fine_tune(\n",
    "        model_id=model_id,\n",
    "        huggingface_cache=huggingface_cache,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=eval_strategy,\n",
    "        logging_strategy=logging_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        use_bf16=use_bf16,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size\n",
    "    ).after(bert_squad_pvc, process_training_data_step, process_validation_data_step)\n",
    "    fine_tune_task.set_accelerator_type(\"nvidia.com/gpu\")\n",
    "    fine_tune_task.set_accelerator_limit(1)\n",
    "    fine_tune_task.set_caching_options(enable_caching=False)\n",
    "\n",
    "    kubernetes.mount_pvc(\n",
    "        download_data_step,\n",
    "        pvc_name=huggingface_pvc.outputs['name'],\n",
    "        mount_path='/huggingface')\n",
    "    kubernetes.mount_pvc(\n",
    "        download_model_step,\n",
    "        pvc_name=huggingface_pvc.outputs['name'],\n",
    "        mount_path='/huggingface')\n",
    "    kubernetes.mount_pvc(\n",
    "        process_training_data_step,\n",
    "        pvc_name=huggingface_pvc.outputs['name'],\n",
    "        mount_path='/huggingface')\n",
    "    kubernetes.mount_pvc(\n",
    "        process_validation_data_step,\n",
    "        pvc_name=huggingface_pvc.outputs['name'],\n",
    "        mount_path='/huggingface')\n",
    "    kubernetes.mount_pvc(\n",
    "        fine_tune_task,\n",
    "        pvc_name=huggingface_pvc.outputs['name'],\n",
    "        mount_path='/huggingface')\n",
    "    kubernetes.mount_pvc(\n",
    "        fine_tune_task,\n",
    "        pvc_name=bert_squad_pvc.outputs['name'],\n",
    "        mount_path='/runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9707e2-92a8-4926-956b-a460ce1410b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(bert_squad_pipeline, package_path='bert-squad-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7108c-fd3c-490a-aee1-83cbcd821c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = client.Client()\n",
    "\n",
    "experiment = client.create_experiment(\n",
    "    name=\"bert-squad-experiment\",\n",
    "    description=\"Fine-tune Bert on the SQuAD dataset, for QA tasks.\",\n",
    "    namespace=NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ae561-2d10-4b52-a3c9-eb73fc8646f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = client.create_run_from_pipeline_package(\n",
    "    pipeline_file=\"bert-squad-pipeline.yaml\",\n",
    "    experiment_name=experiment.display_name,\n",
    "    namespace=NAMESPACE,\n",
    "    run_name=\"bert-squad-run\",\n",
    "    arguments={\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"dataset_id\": \"rajpurkar/squad\",\n",
    "        \"model_id\": \"google-bert/bert-base-uncased\",\n",
    "        \"huggingface_cache\": \"/huggingface\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_steps\": 200,\n",
    "        \"eval_strategy\": \"steps\",\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"use_bf16\": True,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"per_device_eval_batch_size\": 64\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
