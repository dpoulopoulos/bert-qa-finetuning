{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Question-Answering\n",
    "\n",
    "In this Notebook, we fine-tune [BERT (Bidirectional Encoder Representations from Transformers)](https://arxiv.org/abs/1810.04805) for Question Answering (Q&A) tasks using the [SQuAD (Stanford Question Answering)](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html) dataset. Developed by Google in 2018, BERT revolutionized the field of NLP by setting new state-of-the-art benchmarks across various NLP (Natural Language Processing) tasks.\n",
    "\n",
    "BERT is pre-trained on a massive corpus, allowing it to grasp language structure and context. This pre-trained model can then be fine-tuned for specific tasks such as sentiment analysis or question answering. Fine-tuning BERT for Q&A tasks involves adjusting the model to predict the start and end positions of the answer in a given passage for a provided question (extractive question answering). The following steps outline the process of fine-tuning BERT for these tasks:\n",
    "\n",
    "1. **ðŸŒ± Dataset Preparation**:\n",
    "    - Define each dataset item with a question, a passage (or context), and the start and end positions of the answer within the passage as the label.\n",
    "    - Tokenize both the question and passage into subwords using BERT's tokenizer. Separate the question from the passage using the `[SEP]` token and start the input sequence with the `[CLS]`\n",
    "      token.\n",
    "    - Mark the question as segment `A` (or `0`) and the context as segment `B` (or `1`). Use this information to learn different embeddings for each segment, which are added to the word\n",
    "      embeddings.\n",
    "1. **ðŸª¡ Model Modification**:\n",
    "    - Extract embeddings for each token in the sequence from the pre-trained BERT model.\n",
    "    - Add a dense (fully connected) layer on top of BERT, with two output nodes: one for predicting the start position and one for predicting the end position of the answer in the passage (see\n",
    "      [code](https://github.com/huggingface/transformers/blob/c385de24414e4ec6125ee14c46c128bfe70ecb66/src/transformers/models/bert/modeling_bert.py#L1803)).\n",
    "1. **ðŸŽ¯ Training Objective**:\n",
    "    - Output a score for each token in the passage, indicating how likely that token is the start of the answer, and another score for the end.\n",
    "    - Apply a SoftMax function over the sequence to get a probability distribution for the start and end positions.\n",
    "    - Use the sum of the negative log likelihood of the correct start and end positions as the loss function.\n",
    "1. **ðŸš€ Training**:\n",
    "    - Initialize training with pre-trained BERT weights.\n",
    "    - Apply a smaller learning rate (e.g., 2e-5 or 3e-5) since BERT is already pre-trained. Avoid using a larger learning rate, as it may cause the model to diverge.\n",
    "    - Fine-tune the model on the Q&A dataset for several epochs, stopping when validation performance plateaus or decreases.\n",
    "1. **âœ¨ Inference**:\n",
    "    - Tokenize a new question and passage, and add the special `[CLS]` and `[SEP]` tokens.\n",
    "    - Feed the tokens into the fine-tuned BERT model to get scores for the start and end positions of the answer.\n",
    "    - Select the tokens between the predicted start and end positions as the final answer.\n",
    "    - Apply constraints such as ensuring the end position is after the start and limiting the maximum answer length.\n",
    "1. **ðŸŽ‰ Evaluation**:\n",
    "    - Measure performance using common metrics like Exact Match (EM), which calculates the percentage of predictions that exactly match the ground truth and the F1 score to account for partial\n",
    "      matches by considering overlapping words between the prediction and ground truth.\n",
    "\n",
    "Let's begin by implementing each step, one by one, using the Hugging Face ðŸ¤— ecosystem. First, the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from functools import partial\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset , load_metric\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the IDs for the model and the dataset. We will download both of them from the Hugging Face Hub, using the `datasets` and `transformers` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ID = \"rajpurkar/squad\"\n",
    "MODEL_ID = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this section, we will download the dataset, cache it locally, and preprocess it into the format described in the introduction. Our goal is to produce examples that contain:\n",
    "- The tokenized question and context.\n",
    "- The start position of the answer within the context.\n",
    "- The end position of the answer within the context.\n",
    "\n",
    "First, let's download and examine the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load the SQuAD dataset\n",
    "data = load_dataset(DATASET_ID)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has two splits: a `train` split with `87,599` rows, and a `validation` split with `10,570` rows. Each row includes a unique identifier, a title, the context, the question, and one or more possible answers. We will handle each split slightly differently. Youâ€™ll see why later, but for now, let's focus on processing the `train` split.\n",
    "\n",
    "Since we need to tokenize the sequences, let's begin by loading the BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load the BERT tokenizer\n",
    "# set `clean_up_tokenization_spaces` to False to keep the tokenization spaces\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's structure the examples in the desired format. First, we tokenize the questions and the context. Then, using the answer and sequence IDs (or segment IDs, as mentioned in the introduction), we identify the start and end positions of the answer within the context. Finally, we will apply the preprocessing function to each row in the `train` set and discard the columns we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_train_examples(examples, tokenizer, max_length, stride):\n",
    "    \"\"\"Process the training split of the SQuAD dataset.\n",
    "    \n",
    "    Process the training split of the SQuAD dataset to include tokenized questions\n",
    "    and context, as well as the start and end positions of the answer within the context.\n",
    "    \n",
    "    Args:\n",
    "        examples: A row from the dataset containing an example.\n",
    "        tokenizer: The BERT tokenizer to be used.\n",
    "        max_length: The maximum length of the input sequence. If exceeded, truncate the second\n",
    "            sentence of a pair (or a batch of pairs) to fit within the limit.\n",
    "        stride: The number of tokens to retain from the end of a truncated sequence, allowing\n",
    "            for overlap between truncated and overflowing sequences.\n",
    "    \n",
    "    Returns:\n",
    "        The processed example.\n",
    "    \"\"\"\n",
    "    # Tokenize the questions and context sequences\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "      questions,\n",
    "      examples[\"context\"],\n",
    "      truncation=\"only_second\",\n",
    "      padding=\"max_length\",\n",
    "      stride=stride,\n",
    "      max_length=max_length,\n",
    "      return_offsets_mapping=True,\n",
    "      return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    answers = examples[\"answers\"]\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # find the start and end positions of the answer within the context\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_train_data = partial(\n",
    "    preprocess_train_examples, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "processed_train_data = data[\"train\"].map(preprocess_train_data, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "processed_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed `train` split is now ready for fine-tuning BERT. Moving on to model evaluation, the preprocessing step for the `validation` split is almost identical. However, we also need to retain the ID of each row so that we can later evaluate the model's performance by reconstructing the actual answer text and computing the Exact Match (EM) and F1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_valid_examples(examples, tokenizer, max_length, stride):\n",
    "    \"\"\"Process the validation split of the SQuAD dataset.\n",
    "    \n",
    "    Process the training split of the SQuAD dataset to include the unique ID of each row,\n",
    "    the tokenized questions and context, as well as the start and end positions of the answer\n",
    "    within the context.\n",
    "    \n",
    "    Args:\n",
    "        examples: A row from the dataset containing an example.\n",
    "        tokenizer: The BERT tokenizer to be used.\n",
    "        max_length: The maximum length of the input sequence. If exceeded, truncate the second\n",
    "            sentence of a pair (or a batch of pairs) to fit within the limit.\n",
    "        stride: The number of tokens to retain from the end of a truncated sequence, allowing\n",
    "            for overlap between truncated and overflowing sequences.\n",
    "    \n",
    "    Returns:\n",
    "        The processed example.\n",
    "    \"\"\"\n",
    "    # Tokenize the questions and context sequences\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "      questions,\n",
    "      examples[\"context\"],\n",
    "      truncation=\"only_second\",\n",
    "      padding=\"max_length\",\n",
    "      stride=stride,\n",
    "      max_length=max_length,\n",
    "      return_offsets_mapping=True,\n",
    "      return_overflowing_tokens=True,\n",
    "    )\n",
    "    \n",
    "    example_ids = []\n",
    "    answers = examples[\"answers\"]\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # find the start and end positions of the answer within the context\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids  # keep the unique ID of the example\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_valid_data = partial(\n",
    "    preprocess_valid_examples, tokenizer=tokenizer, max_length=384, stride=128)\n",
    "processed_valid_data = data[\"validation\"].map(preprocess_valid_data, batched=True, remove_columns=data[\"validation\"].column_names)\n",
    "processed_valid_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
